# 第5章 马尔可夫过程

## 本章导读

马尔可夫过程是一类具有"无记忆性"的随机过程，在机器人状态估计、强化学习等领域有广泛应用。本章系统介绍马尔可夫链、连续时间马尔可夫过程和马尔可夫决策过程等核心内容。通过本章学习，你将掌握马尔可夫过程的数学理论，理解其在机器人路径规划和决策中的应用。

---

## 5.1 马尔可夫链

### 5.1.1 马尔可夫链的定义

定义 5.1（马尔可夫链） 设 $\{X_n\}_{n=0}^\infty$ 是离散时间随机过程，状态空间为 $S$（有限或可数）。若满足**马尔可夫性**：
$$P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1}, \ldots, X_0 = i_0) = P(X_{n+1} = j | X_n = i)$$

则称 $\{X_n\}$ 为**马尔可夫链**（Markov Chain）。

**直观解释**：给定当前状态，未来与过去独立。

### 5.1.2 转移概率

定义 5.2（一步转移概率） **一步转移概率**定义为：
$$p_{ij} = P(X_{n+1} = j | X_n = i)$$

**转移概率矩阵**

$$P = (p_{ij}) = \begin{pmatrix} p_{11} & p_{12} & \cdots \\ p_{21} & p_{22} & \cdots \\ \vdots & \vdots & \ddots \end{pmatrix}$$

性质：
- $p_{ij} \geq 0$
- $\sum_j p_{ij} = 1$（每行和为1）

### 5.1.3 n步转移概率

定理 5.1（Chapman-Kolmogorov方程）
$$p_{ij}^{(n+m)} = \sum_k p_{ik}^{(n)} p_{kj}^{(m)}$$

矩阵形式：
$$P^{(n)} = P^n$$

### 5.1.4 状态分类

**可达与互通**

定义 5.3（可达） 状态 $j$ **可达**（accessible）于状态 $i$，如果存在 $n \geq 0$ 使得 $p_{ij}^{(n)} > 0$。记为 $i \to j$。

定义 5.4（互通） 若 $i \to j$ 且 $j \to i$，则称 $i$ 和 $j$ **互通**（communicate）。记为 $i \leftrightarrow j$。

**状态的周期性**

定义 5.5（周期） 状态 $i$ 的**周期**定义为：
$$d(i) = \gcd\{n \geq 1 : p_{ii}^{(n)} > 0\}$$

若 $d(i) = 1$，称状态 $i$ 是**非周期的**（aperiodic）。

**常返与暂态**

定义 5.6（首达概率） 从 $i$ 出发首次到达 $j$ 的概率：
$$f_{ij}^{(n)} = P(X_n = j, X_k \neq j, 1 \leq k \leq n-1 | X_0 = i)$$

定义 5.7（常返与暂态）
- 若 $\sum_{n=1}^\infty f_{ii}^{(n)} = 1$，称状态 $i$ 是**常返的**（recurrent）
- 若 $\sum_{n=1}^\infty f_{ii}^{(n)} < 1$，称状态 $i$ 是**暂态的**（transient）

### 5.1.5 平稳分布

定义 5.8（平稳分布） 概率分布 $\pi = (\pi_1, \pi_2, \ldots)$ 称为**平稳分布**，如果：
$$\pi = \pi P$$

即 $\pi_j = \sum_i \pi_i p_{ij}$ 对所有 $j$ 成立。

**存在唯一性定理**

定理 5.2 对于不可约、非周期的有限马尔可夫链：
- 存在唯一的平稳分布 $\pi$
- $\lim_{n \to \infty} p_{ij}^{(n)} = \pi_j$ 对所有 $i$ 成立

---

## 5.2 连续时间马尔可夫过程

### 5.2.1 定义与性质

定义 5.9（连续时间马尔可夫过程） 连续时间随机过程 $\{X(t)\}_{t \geq 0}$ 具有**马尔可夫性**，如果：
$$P(X(t+s) = j | X(s) = i, X(u), 0 \leq u < s) = P(X(t+s) = j | X(s) = i)$$

### 5.2.2 转移速率

定义 5.10（转移速率） **转移速率**（infinitesimal generator）定义为：
$$q_{ij} = \lim_{h \to 0} \frac{p_{ij}(h)}{h}, \quad i \neq j$$

$$q_{ii} = -\sum_{j \neq i} q_{ij}$$

**转移速率矩阵** $Q = (q_{ij})$：
- 非对角元 $q_{ij} \geq 0$（$i \neq j$）
- 每行和为0

### 5.2.3 Kolmogorov方程

**向前方程**
$$\frac{dP(t)}{dt} = P(t)Q$$

**向后方程**
$$\frac{dP(t)}{dt} = QP(t)$$

解：$P(t) = e^{Qt}$

### 5.2.4 生灭过程

定义 5.11（生灭过程） **生灭过程**是状态空间为 $\{0, 1, 2, \ldots\}$ 的连续时间马尔可夫过程，只能转移到相邻状态：
- 出生率：$q_{i,i+1} = \lambda_i$
- 死亡率：$q_{i,i-1} = \mu_i$
- $q_{ii} = -(\lambda_i + \mu_i)$

**泊松过程是特殊的纯生过程**

---

## 5.3 马尔可夫决策过程

### 5.3.1 MDP的定义

定义 5.12（马尔可夫决策过程） **马尔可夫决策过程**（MDP）由五元组 $(S, A, P, R, \gamma)$ 定义：
- $S$：状态空间
- $A$：动作空间
- $P(s'|s,a)$：状态转移概率
- $R(s,a,s')$：奖励函数
- $\gamma \in [0,1]$：折扣因子

### 5.3.2 策略与价值函数

定义 5.13（策略） **策略** $\pi$ 是从状态到动作的映射：
- 确定性策略：$a = \pi(s)$
- 随机策略：$\pi(a|s)$

定义 5.14（状态价值函数） **状态价值函数**：
$$V^\pi(s) = E\left[\sum_{t=0}^\infty \gamma^t R_t | S_0 = s, \pi\right]$$

定义 5.15（动作价值函数） **动作价值函数**（Q函数）：
$$Q^\pi(s,a) = E\left[\sum_{t=0}^\infty \gamma^t R_t | S_0 = s, A_0 = a, \pi\right]$$

### 5.3.3 Bellman方程

**Bellman期望方程**
$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]$$

$$Q^\pi(s,a) = \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma \sum_{a'} \pi(a'|s')Q^\pi(s',a')]$$

**Bellman最优方程**
$$V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^*(s')]$$

$$Q^*(s,a) = \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma \max_{a'} Q^*(s',a')]$$

### 5.3.4 求解算法

**值迭代**

算法 5.1（值迭代）

1. 初始化 $V_0(s) = 0$ 对所有 $s$
2. **循环**直到收敛：
   $$V_{k+1}(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V_k(s')]$$
3. **返回** $V^* = V_k$ 和最优策略 $\pi^*(s) = \arg\max_a \ldots$

**策略迭代**

算法 5.2（策略迭代）

1. 初始化策略 $\pi_0$
2. **循环**直到策略不变：
   - **策略评估**：计算 $V^{\pi_k}$
   - **策略改进**：$\pi_{k+1}(s) = \arg\max_a Q^{\pi_k}(s,a)$
3. **返回** $\pi^*$

---

## 5.4 马尔可夫过程在机器人中的应用

### 5.4.1 机器人定位

**问题描述**

机器人在环境中移动，通过传感器观测估计自身位置。

**建模为HMM**
- 隐藏状态：机器人位置
- 观测：传感器读数
- 转移：运动模型

### 5.4.2 路径规划

**MDP建模**
- 状态：机器人位置/构型
- 动作：移动方向
- 奖励：到达目标的负距离、碰撞惩罚

### 5.4.3 强化学习

**Q-Learning**

算法 5.3（Q-Learning）

1. 初始化 $Q(s,a) = 0$
2. **循环**（每个episode）：
   - 观察当前状态 $s$
   - 选择动作 $a$（$\epsilon$-greedy）
   - 执行动作，观察奖励 $r$ 和下一状态 $s'$
   - 更新：$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$
3. **返回** $Q$

---

## 5.5 本章小结

本章介绍了马尔可夫过程的理论和应用：

1. **马尔可夫链**：
   - 马尔可夫性：未来只依赖于现在
   - 转移概率矩阵
   - 状态分类：常返、暂态、周期
   - 平稳分布

2. **连续时间马尔可夫过程**：
   - 转移速率矩阵
   - Kolmogorov方程
   - 生灭过程

3. **马尔可夫决策过程**：
   - MDP定义
   - 价值函数和Bellman方程
   - 值迭代和策略迭代

4. **机器人应用**：
   - 定位（HMM）
   - 路径规划（MDP）
   - 强化学习（Q-Learning）

马尔可夫过程为机器人决策和状态估计提供了强大的数学工具。

---

## 习题

### 基础题

5.1 证明：对于马尔可夫链，$P^{(n)} = P^n$。

5.2 判断给定马尔可夫链的状态类型（常返/暂态）。

5.3 求给定转移矩阵的平稳分布。

5.4 推导连续时间马尔可夫过程的Kolmogorov向前方程。

### 提高题

5.5 证明不可约有限马尔可夫链存在唯一平稳分布。

5.6 分析生灭过程的平稳分布存在的条件。

5.7 证明值迭代的收敛性。

5.8 设计一个机器人路径规划的MDP模型。

### 编程题

5.9 实现值迭代算法，求解Grid World问题。

5.10 实现Q-Learning，训练机器人避障。

---

## 参考文献

1. Ross S M. Introduction to Probability Models[M]. Academic Press, 2014.
2. Puterman M L. Markov Decision Processes: Discrete Stochastic Dynamic Programming[M]. Wiley, 1994.
3. Sutton R S, Barto A G. Reinforcement Learning: An Introduction[M]. MIT Press, 2018.
4. Thrun S, et al. Probabilistic Robotics[M]. MIT Press, 2005.
